{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{"_uuid":"0d4769f8-3af7-400e-b4f6-12e5a007929a","_cell_guid":"6bf7a6ff-1238-4185-86e1-d1962de3c0d9","trusted":true}},{"cell_type":"markdown","source":"***Objective:*** Binary Clasification of Image data. To classify an image as belonging to one of the 2 classes: Dog or Cat\n\n***Concepts:*** Convolutional Neural Network (CNN/ConvNet) implemented using Keras API of Tensorflow\n\n***Level***: Beginner Friendly. It consolidates all my learning from various sources. Please feel free to Upvote if this notebook helps you. Such small little things do motivate us :) I will be adding more functionality to this notebook in the future.","metadata":{"_uuid":"e8de7d9c-3e3e-4e66-8b86-b9bb66261772","_cell_guid":"8873d8a9-77e9-4334-b8dd-b195fe8e8c76","trusted":true}},{"cell_type":"markdown","source":"## Table of Contents","metadata":{"_uuid":"d2a54063-39ad-40d3-804e-2e149969ddb9","_cell_guid":"d3e3c81e-a05b-4376-bcdc-9c8de32ba4cf","trusted":true}},{"cell_type":"markdown","source":"- [Libraries and Utilities](#lib) \n- [Define Constants](#cons)\n- [Load Data](#load)\n- [Data Exploration](#exp)\n- [Model Training](#train)\n- [Callbacks](#call)\n- [Data Augmentation](#aug)\n- [Fit Model](#fit)\n- [Model Evaluation](#eval)\n- [Misclassified Images Analysis](#miss)","metadata":{"_uuid":"e7842245-7395-42bd-8199-4ad13f1563c3","_cell_guid":"adead87d-73ce-493f-a828-ca7988b6c39f","trusted":true}},{"cell_type":"markdown","source":"## Import Libraries\n<a id= \"lib\"> </a>","metadata":{"_uuid":"bbdc4ada-fc6c-472c-84ed-00eae646aa9b","_cell_guid":"43a4b19e-32db-40f9-9561-9d2eb26d1f9b","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom tqdm import tqdm #to create progress bar\nimport cv2 \nimport os\nprint(os.listdir(\"../input\"))","metadata":{"_uuid":"7076255e-a0da-4d8d-beaf-0fe7fad98979","_cell_guid":"6d4592c2-7c94-4dfc-999f-7c7041a6f0e2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T07:58:10.406027Z","iopub.execute_input":"2024-03-10T07:58:10.406594Z","iopub.status.idle":"2024-03-10T07:58:11.499672Z","shell.execute_reply.started":"2024-03-10T07:58:10.406465Z","shell.execute_reply":"2024-03-10T07:58:11.498536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","metadata":{"_uuid":"c67eb42e-f0a0-49ff-8b2f-f12110995e49","_cell_guid":"158474d5-9ae3-4720-9974-fbd4e928a59d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T07:58:19.190842Z","iopub.execute_input":"2024-03-10T07:58:19.191698Z","iopub.status.idle":"2024-03-10T07:58:23.803956Z","shell.execute_reply.started":"2024-03-10T07:58:19.191654Z","shell.execute_reply":"2024-03-10T07:58:23.802808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Constants\n<a id= \"cons\"> </a>","metadata":{"_uuid":"450fb645-ae3a-4daa-b0fd-9a86d3bd7b8c","_cell_guid":"e75cd554-c161-4730-bc32-3890487b794a","trusted":true}},{"cell_type":"code","source":"FAST_RUN = False\nIMAGE_WIDTH=224\nIMAGE_HEIGHT=224\nIMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\nIMAGE_CHANNELS=3 # 3 for Colored Images and 1 for Grayscale Images\nBATCH_SIZE=32\nEPOCHS= 5\n\nif FAST_RUN:\n    EPOCHS= 5","metadata":{"_uuid":"2e6a23d7-1742-4e70-a700-743592fe8904","_cell_guid":"4f56bb54-a051-43f2-a7e7-6ae462193aad","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T07:58:25.343703Z","iopub.execute_input":"2024-03-10T07:58:25.344970Z","iopub.status.idle":"2024-03-10T07:58:25.350221Z","shell.execute_reply.started":"2024-03-10T07:58:25.344926Z","shell.execute_reply":"2024-03-10T07:58:25.349135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['Fake',\"Real\"]\nclass_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n\nnb_classes = len(class_names)","metadata":{"_uuid":"ed54617a-256a-4f35-930d-79e57694b91e","_cell_guid":"d260c0ad-5c8d-4741-9ffb-d68109cbe2b2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T07:58:29.785249Z","iopub.execute_input":"2024-03-10T07:58:29.786083Z","iopub.status.idle":"2024-03-10T07:58:29.791371Z","shell.execute_reply.started":"2024-03-10T07:58:29.786038Z","shell.execute_reply":"2024-03-10T07:58:29.790237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names_label","metadata":{"_uuid":"528522d7-74c0-4395-80df-9d00fe028d96","_cell_guid":"c32f6109-436f-47f5-88d5-407676d36f2d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T07:58:32.115625Z","iopub.execute_input":"2024-03-10T07:58:32.116040Z","iopub.status.idle":"2024-03-10T07:58:32.125539Z","shell.execute_reply.started":"2024-03-10T07:58:32.116005Z","shell.execute_reply":"2024-03-10T07:58:32.124387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data\n<a id= \"load\"> </a>","metadata":{"_uuid":"26049030-2517-4958-b641-ec98ddc65024","_cell_guid":"5481f883-d2ff-41e8-b371-c96a09bee2d9","trusted":true}},{"cell_type":"code","source":"train_data_dir = '/kaggle/input/deepfake-and-real-images/Dataset/Train'\ntest_data_dir = '/kaggle/input/deepfake-and-real-images/Dataset/Test'","metadata":{"_uuid":"0c921a3f-6a4c-404d-8f08-494e5358c852","_cell_guid":"29f38922-9812-4d3f-92af-d89ad8b57de0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T08:04:33.115646Z","iopub.execute_input":"2024-03-10T08:04:33.116714Z","iopub.status.idle":"2024-03-10T08:04:33.121544Z","shell.execute_reply.started":"2024-03-10T08:04:33.116672Z","shell.execute_reply":"2024-03-10T08:04:33.120395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    shear_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\ntest_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)","metadata":{"_uuid":"e644873f-b257-4752-bf26-a34ef8c50256","_cell_guid":"8da7c6bc-f6f2-481e-97f2-eb8aa8d0455c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T08:05:39.089684Z","iopub.execute_input":"2024-03-10T08:05:39.090093Z","iopub.status.idle":"2024-03-10T08:06:47.831464Z","shell.execute_reply.started":"2024-03-10T08:05:39.090058Z","shell.execute_reply":"2024-03-10T08:06:47.830230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data contains consecutive images of dogs followed by images of cats together. Let us shuffle it.","metadata":{"_uuid":"40d7a716-04e1-4ee1-a87f-e3341e6de1c2","_cell_guid":"6c1eca06-45ee-4815-9874-24058053687f","trusted":true}},{"cell_type":"markdown","source":"## Explore Data\n<a id= \"exp\"> </a>","metadata":{"_uuid":"9df5bea2-bf6b-4bc0-bedc-63624e8672b5","_cell_guid":"3bbfad2b-ed48-4b47-93f4-f9bfec7b2ecd","trusted":true}},{"cell_type":"markdown","source":"Let us get the length of each dataset, counts across all categories etc.","metadata":{"_uuid":"3eb9a87e-25e2-419e-882b-35795a916a89","_cell_guid":"c7e35c2a-2157-4147-b803-fb014467da3e","trusted":true}},{"cell_type":"markdown","source":"### Plot Sample Image","metadata":{"_uuid":"63cc8cd1-47be-4bdf-b953-64defc6c3806","_cell_guid":"084310c0-e184-4f48-a47f-3fc9ddc1c191","trusted":true}},{"cell_type":"markdown","source":"### Some data-format related changes to make datasets comptabile with Keras","metadata":{"_uuid":"5ab698bb-e950-4a80-8ed5-7eeee085841c","_cell_guid":"64a8a8db-32e6-4b48-8638-a041c3e82842","trusted":true}},{"cell_type":"markdown","source":"Using the method to_categorical(), `train_labels` which has categories represented by integers (1 for Dog, 0 for Cat) is converted into a matrix (eg for Dog: [0,1] and for Cat: [1,0]).<br>\nOutput: \nThis function returns a matrix of binary values (either ‘1’ or ‘0’). It has number of rows equal to the length of the input vector and number of columns equal to the number of classes.","metadata":{"_uuid":"85aeca76-5af8-4067-a5fc-02fc64b67492","_cell_guid":"1f87333f-e249-4516-9a63-93494af789e4","trusted":true}},{"cell_type":"markdown","source":"train_images is already in Keras accepted format (number_of_images, image width, image height, channels). So we do not need to do reshape it again. <br>\np.s. if number of images is unknown during the reshape operation for number_of_images parameter, put `-1` instead.","metadata":{"_uuid":"7fd0b832-32a0-4e2c-a07c-e08488764740","_cell_guid":"28eb9756-ef50-45b2-af6f-b47d8032ae80","trusted":true}},{"cell_type":"markdown","source":"## Model Training\n<a id= \"train\"> </a>","metadata":{"_uuid":"bb237597-223f-455f-83fe-798ed8629b53","_cell_guid":"5d085636-1d1d-4cdf-9d78-d5ac50dd36bf","trusted":true}},{"cell_type":"markdown","source":"**Layers**<br>\nIn Keras Sequential API, we add one layer at a time, starting from the input layer.\n- **Conv2D:** 2D convolution layer. When using this layer as the first layer in a model, provide the keyword argument input_shape, e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\". You can use None when a dimension has variable size.<br>\nOther key arguments are `filters` and `kernel_size`. filters is the number of desired feature maps. kernel_size is the size of the convolution window. It can be an integer or tuple/list of 2 integers (eg 5 or (5,5) mean the same thing for kernel_size). This layer extracts features from an image (eg ear shape, eyes shape etc in case of a Cat image). \n- **MaxPooling2D:** Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by `strides` along each dimension.\n- **Flatten:** Flattens the input, that is converts the final feature maps into a one single 1D vector.. Does not affect the batch size.\n- **Dense:** Just your regular densely-connected NN layer.\n- **Dropout:** This layer is used for Regularization. It randomly sets input units to 0 with a frequency of `rate` at each step during training time, which helps prevent overfitting. <br>\nrate: Float between 0 and 1. Fraction of the input units to drop.\n- **BatchNormalization:** Layer that normalizes its inputs. Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.","metadata":{"_uuid":"ad7ec40a-1ca1-4f17-8e71-c14a35d124db","_cell_guid":"2cd6907e-b31c-4c84-88e8-f329a04bc904","trusted":true}},{"cell_type":"markdown","source":"**Activation Functions:**\n\n- **Relu:** 'relu' is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network. Given a value x, returns max(x, 0).\n\n- **Softmax:** 2 neurons, probability that the image belongs to one of the two classes. Softmax converts a vector of values to a probability distribution. The elements of the output vector are in range (0, 1) and sum to 1. Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution.","metadata":{"_uuid":"3b22e852-ac4f-4966-acc1-f237fb5c4688","_cell_guid":"72456b43-02dc-41a3-8d6e-931a8d4816c3","trusted":true}},{"cell_type":"markdown","source":"**Loss and Optimizer**<br>\n- Binary Cross Entropy computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss for binary (0 or 1) classification applications. Since we have only 2 classes, we used `binary_crossentropy`\n- CategoricalCrossentropy: Use this crossentropy loss function when there are two or more label classes. The labels are expected to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss.\n- RMSprop: The gist of RMSprop is to: Maintain a moving (discounted) average of the square of gradients, Divide the gradient by the root of this average. For our project, we have used `rmsprop` optimizer. Feel free to apply adam or other optimizers and see if it improves the performance of the model.\n- adam: Another popular optimizer is adam. Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.","metadata":{"_uuid":"60130854-6804-415f-b02d-dd2f396aad63","_cell_guid":"a504acda-be16-4d46-908b-1a6e709f8403","trusted":true}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n#model.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes\n\nmodel.compile(loss='binary_crossentropy', optimizer=\"adam\",metrics=['accuracy'])\n\nmodel.summary()","metadata":{"_uuid":"a71c68ff-bd21-4c92-ac25-93a2f395730e","_cell_guid":"b84a74bf-769e-4972-a00c-53545933eef8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T08:07:44.844144Z","iopub.execute_input":"2024-03-10T08:07:44.844610Z","iopub.status.idle":"2024-03-10T08:07:47.704078Z","shell.execute_reply.started":"2024-03-10T08:07:44.844568Z","shell.execute_reply":"2024-03-10T08:07:47.703061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Callbacks\n<a id= \"call\"> </a>","metadata":{"_uuid":"b56d65fe-8845-4659-b326-575457efadb4","_cell_guid":"b67a9f3d-f677-46de-8b54-9ebabca90c21","trusted":true}},{"cell_type":"markdown","source":"**Early Stopping**\n\nTo prevent over fitting we will stop the learning after val_loss value has not decreased for 10 epochs","metadata":{"_uuid":"0e2f1c6d-d08c-4297-9627-f92acb12b5c6","_cell_guid":"55d5520b-a7e6-43b7-a2ec-2ddd588d53a7","trusted":true}},{"cell_type":"code","source":"earlystop = EarlyStopping(patience=10)","metadata":{"_uuid":"e5a844b0-b9d1-44fe-a2e3-b3064a019357","_cell_guid":"3f95c38c-d4f7-4deb-95ce-742e5c7efa83","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T08:07:51.477686Z","iopub.execute_input":"2024-03-10T08:07:51.478578Z","iopub.status.idle":"2024-03-10T08:07:51.483514Z","shell.execute_reply.started":"2024-03-10T08:07:51.478524Z","shell.execute_reply":"2024-03-10T08:07:51.482363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Learning Rate Reduction** <br>\ntl;dr: We will reduce the Learning Rate when Accuracy does not increase for 2 steps. <br>\nThe Learning Rate (LR) is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n\nIt is better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function. To keep the advantage of the fast computation time with a high LR, we will decrease the LR dynamically every X steps (epochs) depending on if it is necessary (when accuracy does not improve).\n\nWith the ReduceLROnPlateau function from Keras.callbacks, we will reduce the LR by `factor` (0.1 in this case) if the accuracy does not improve after 2 epochs.","metadata":{"_uuid":"9a1d25cb-5434-4c4e-a7ba-713d9dc27fca","_cell_guid":"92334c5a-4e96-4104-a7be-266b03c069a9","trusted":true}},{"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.1, \n                                            min_lr=0.00001)","metadata":{"_uuid":"7eaeabae-6d25-419f-a405-7184c02ef985","_cell_guid":"6a29b5b3-7a56-4c2e-aeac-820bc28d96dd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T08:07:53.350553Z","iopub.execute_input":"2024-03-10T08:07:53.351493Z","iopub.status.idle":"2024-03-10T08:07:53.356505Z","shell.execute_reply.started":"2024-03-10T08:07:53.351456Z","shell.execute_reply":"2024-03-10T08:07:53.355428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [earlystop, learning_rate_reduction]","metadata":{"_uuid":"78744ee5-cec9-4d7d-97e4-7f29678fba87","_cell_guid":"4d23d283-2ff7-4829-9071-78d9cf899e1a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T08:07:55.480762Z","iopub.execute_input":"2024-03-10T08:07:55.481746Z","iopub.status.idle":"2024-03-10T08:07:55.486214Z","shell.execute_reply.started":"2024-03-10T08:07:55.481704Z","shell.execute_reply":"2024-03-10T08:07:55.485085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation to Prevent Overfitting\n<a id= \"aug\"> </a>","metadata":{"_uuid":"c5a970fc-1ebf-4e3b-be58-842d10f51dc9","_cell_guid":"a58da7b4-d4ca-408a-9e6c-2cec5917f61d","trusted":true}},{"cell_type":"markdown","source":"Data Augmentation is done to prevent overfitting by exposing the images to random changes like Resizing, Flips, Rotation etc. This is what happens during Augmentation:\n- A batch of images is taken for training. \n- The Generator applies random transformations to each image in the batch during Training.\n- Replacing the original batch of images with a new randomly transformed batch.\n- During each Epoch, a random variation of the augmented image is used for training.<br><br>\nKey Points:\n- The overall number of sample size does not increase or decrease due to Data Augmentation\n- Augmentation prevents model from generalising. Instead of learning too much from an image, it learns from an augmented transformation in each epoch\n- `flow()` , `flow_from_directory()`, `flow_from_dataframe()` : One of these three functions can be used to transform Images. When there are separate subfolders for each category (eg cat images in Cat folder, and Dog images in Dog folder), then we use `flow_from_directory()`. If there is a single folder which contains all the images, then we can use `flow_from_dataframe()`. \n- `fit()` :Fits the data generator to some sample data. This computes the internal data stats related to the data-dependent transformations, based on an array of sample data. Only required if featurewise_center or featurewise_std_normalization or zca_whitening are set to True. When rescale is set to a value, rescaling is applied to sample data before computing the internal data stats.\n- Data Augmentation techniques are applied only to the data we train the model on (train_images), we do not apply it on test dataset (test_images) or validation dataset (in case there are 3 sets).","metadata":{"_uuid":"5fbd1328-10f3-4e41-8194-ce503e585640","_cell_guid":"9b15fb79-0093-4a34-9f94-d79f310c24b5","trusted":true}},{"cell_type":"markdown","source":"**Training Generator**","metadata":{"_uuid":"7f274ee9-5209-4038-b1c5-9b61261b2fe3","_cell_guid":"1991723d-5286-4a72-970e-bc7cb5aeac74","trusted":true}},{"cell_type":"markdown","source":"For Training Images, we will apply following augmentations:\n\n- Randomly rotate some training images by 15 degrees\n- Randomly Zoom by 20% some training images\n- Horizontally flip images \n- Randomly shift images horizontally by 10% of the width\n- Randomly shift images vertically by 10% of the height","metadata":{"_uuid":"5b297816-e08c-441d-b97b-0df93aa9713d","_cell_guid":"7e72d32e-d030-4e28-8094-615fcd91ac49","trusted":true}},{"cell_type":"markdown","source":"**Validation Generator**","metadata":{"_uuid":"b87df10b-101b-41de-8ddd-157a6c8d9b89","_cell_guid":"1d9e4a7f-9eed-42ab-babc-823bd5873366","trusted":true}},{"cell_type":"markdown","source":"For Validation images, we will only apply Normalization. <br>\nNormalization ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network. <br>\nA good read to understand the importance of Image Normalization:<br>\nhttps://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current","metadata":{"_uuid":"adf0a135-c6b8-480d-857e-408d34ce390e","_cell_guid":"8a15050a-2b73-4ad3-b655-2d55f7755bc5","trusted":true}},{"cell_type":"markdown","source":"## Check Sample Image after augmentation","metadata":{"_uuid":"5a258580-4be8-4890-9a68-3ae76098003f","_cell_guid":"48eb1b82-e3ec-405e-acc9-1507b9380f77","trusted":true}},{"cell_type":"markdown","source":"## Fit Model\n<a id= \"fit\"> </a>","metadata":{"_uuid":"16ef21a4-b705-4cc0-94e3-97c0e4d4c1c6","_cell_guid":"8958ca27-6ad0-4ec4-9947-e2759e5f155f","trusted":true}},{"cell_type":"markdown","source":"- When using Generator for inputs, we do not need to specify (train_X, train_Y) separately. The model will automatically take train_Y from Generator objects. It aplies for both training and validation datasets. If we are not using Generator or Dataframe, then we need to supply input in the form (train_X, train_Y)\n- steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined.\n- If input to fit() is generator, then we cannot use validation_split. Therefore, we need to have separate train and validation sets for the CNN fit() method which we can achieve either by reading train-test datasets from separate directories (like in our case) or using sklearn train_test_split method.","metadata":{"_uuid":"e4a9b305-950c-4413-a7ce-e7065727559b","_cell_guid":"0139bf38-d8b8-4360-956f-94e3f6a490d3","trusted":true}},{"cell_type":"code","source":"hist = model.fit(\n    train_generator,\n    epochs=5,\n    validation_data=test_generator,\n    callbacks=callbacks\n)","metadata":{"_uuid":"458d8865-3b3d-4427-9d15-44b6d5451e9c","_cell_guid":"8b9469af-5a45-4c18-b48f-c50f8d170b98","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T08:23:30.730106Z","iopub.execute_input":"2024-03-10T08:23:30.730794Z","iopub.status.idle":"2024-03-10T09:53:22.649508Z","shell.execute_reply.started":"2024-03-10T08:23:30.730753Z","shell.execute_reply":"2024-03-10T09:53:22.648601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the Model","metadata":{"_uuid":"dcccfca0-b2b6-4a78-a5fb-83b6bc99c417","_cell_guid":"a58b9e5a-5248-44d6-aa5e-5da5de97fcb0","trusted":true}},{"cell_type":"code","source":"model.save('deepdetectv4.h5')","metadata":{"_uuid":"2d8a7724-d848-458b-85bd-4f6fcda9693c","_cell_guid":"df02cf1e-b314-4815-a099-e9e430e2d9c6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T09:53:22.711237Z","iopub.execute_input":"2024-03-10T09:53:22.711598Z","iopub.status.idle":"2024-03-10T09:53:22.779928Z","shell.execute_reply.started":"2024-03-10T09:53:22.711567Z","shell.execute_reply":"2024-03-10T09:53:22.778922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation\n<a id= \"eval\"> </a>","metadata":{"_uuid":"4e952c75-60d5-40b5-9fec-ee176cc52571","_cell_guid":"1c7e4d37-b772-47b6-b87d-20eac63f4fa0","trusted":true}},{"cell_type":"code","source":"from tensorflow import keras \nmodel3= keras.models.load_model(\"/kaggle/working/deepdetectv3.h5\")","metadata":{"_uuid":"9097c7a9-c1b0-42d3-936b-ce0dd5888f75","_cell_guid":"6ee94502-64be-497b-9ca5-505327cad380","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T10:02:52.992728Z","iopub.execute_input":"2024-03-10T10:02:52.993178Z","iopub.status.idle":"2024-03-10T10:02:53.122155Z","shell.execute_reply.started":"2024-03-10T10:02:52.993123Z","shell.execute_reply":"2024-03-10T10:02:53.120978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting Loss and Accuracy graphs for Training and Validation datasets","metadata":{"_uuid":"af204511-0824-4f75-89ea-71a9ddec7141","_cell_guid":"5634d8d5-47d8-4cad-80ec-178a2c492229","trusted":true}},{"cell_type":"markdown","source":"   **Some of the places from where I learnt CNN which made this notebook possible:**<br>\nhttps://www.kaggle.com/code/yassineghouzam/introduction-to-cnn-keras-0-997-top-6 <br>\nhttps://www.kaggle.com/code/uysimty/keras-cnn-dog-or-cat-classification <br>\nhttps://www.kaggle.com/code/rajmehra03/flower-recognition-cnn-keras <br>\nhttps://www.kaggle.com/code/vincee/intel-image-classification-cnn-keras/notebook <br>\nhttps://www.geeksforgeeks.org/ <br>","metadata":{"_uuid":"0986e69c-6336-4f6e-860c-c45094afc536","_cell_guid":"8d930436-fd7f-49da-b99c-22ab4f96c915","execution":{"iopub.status.busy":"2022-07-06T15:23:34.281672Z","iopub.execute_input":"2022-07-06T15:23:34.282845Z","iopub.status.idle":"2022-07-06T15:23:34.290538Z","shell.execute_reply.started":"2022-07-06T15:23:34.282792Z","shell.execute_reply":"2022-07-06T15:23:34.289229Z"},"trusted":true}},{"cell_type":"code","source":"model3.predict(\"/kaggle/input/deepfake-and-real-images/Dataset/Validation/Real/real_10.jpg\")","metadata":{"_uuid":"4f5c0a70-7837-411c-bf1c-9a980ef7a3ec","_cell_guid":"97749f97-85d7-4e58-8d5a-26fb127ae1b3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-10T10:11:33.226996Z","iopub.execute_input":"2024-03-10T10:11:33.228113Z","iopub.status.idle":"2024-03-10T10:11:33.274164Z","shell.execute_reply.started":"2024-03-10T10:11:33.228063Z","shell.execute_reply":"2024-03-10T10:11:33.272529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" [code]","metadata":{"_uuid":"d56b3e4c-46e8-4ece-ad05-d9f608020e65","_cell_guid":"24de5c68-4cc0-4a01-be95-cd4478358576","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}